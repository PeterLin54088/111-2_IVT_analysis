{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477435fb-45f6-4f80-bf8a-c6f7499a3681",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd011df-b141-4fa5-8380-dba0b177ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "\"\"\"\n",
    "# Tensor\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8959087-bf41-44f6-9f60-a13c23975cee",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "834e4edf-f0c2-4e4a-b6f5-1d2cb81e9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config path\n",
    "root = '/Volumes/Expansion/User_Backup/b08209033/111-2_IVT_analysis/'\n",
    "file = 'config.json'\n",
    "config_path = os.path.join(root, file)\n",
    "\n",
    "# Import config\n",
    "with open(config_path) as infile:\n",
    "    config = json.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "# Update config\n",
    "config.update({\"Flag_timeline_feature\": True})\n",
    "config.update({\"ML_fname_dataset\": \"IVT_TS_dataset.npz\"})\n",
    "\n",
    "# Export config\n",
    "with open(config_path, 'w') as outfile:\n",
    "    json.dump(config, outfile, sort_keys=True)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f14cc-a6b3-4bee-8c63-98d9b0ba0fed",
   "metadata": {},
   "source": [
    "# Read time structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af15f15-4dcd-4851-92ce-73b10f85c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(config[\"Path_IVT_calculation\"])\n",
    "with np.load(config[\"Fname_IVT_svd\"]) as dataset:\n",
    "    time_structure = dataset['time']\n",
    "    feature_num = int(config[\"Var_Feature_num_SVD\"])\n",
    "# Choose k important structure\n",
    "time_structure = (time_structure[:feature_num]).T # (Timestep, feature) after transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5885d50-7b49-4900-ae88-7d7c2fb107f0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4deb2116-5e58-4b1f-8e71-dcc2db7d7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "# Normalizer\n",
    "# Split observation and prediction\n",
    "# Pack to DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea7c25-1d18-49a2-bf58-32bc51715651",
   "metadata": {},
   "source": [
    "### Define data split method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab860e3-7d0c-4116-9265-3b1ebc95b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio\n",
    "    # Total: 16 partitions\n",
    "    # train: 12 partitions\n",
    "    # test :  3 partitions\n",
    "    # valid:  1 partitions\n",
    "data_ratio  = 16/16\n",
    "train_ratio = 12/16\n",
    "test_ratio  =  3/16\n",
    "valid_ratio =  1/16\n",
    "split_ratio = [train_ratio, valid_ratio, test_ratio]\n",
    "\n",
    "# Size\n",
    "data_size  = len(time_structure)\n",
    "train_size = int(data_size * train_ratio)\n",
    "test_size  = int(data_size * test_ratio)\n",
    "valid_size = data_size - (train_size + test_size) # remaining\n",
    "split_size = [train_size, valid_size, test_size]\n",
    "\n",
    "# Save data split info\n",
    "config.update({\"ML_split_ratio\": split_ratio})\n",
    "config.update({\"ML_split_size\":  split_size })\n",
    "with open(config_path, 'w') as outfile:\n",
    "    json.dump(config, outfile, sort_keys=True)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44b293-7565-42bb-b5e7-dd5dfa455540",
   "metadata": {},
   "source": [
    "### Timeline feature (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3add2c1-9c8c-4506-acee-a866a5cf3197",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline_addfeat = np.arange(data_size) # monotonic0\n",
    "timeline_addfeat = np.sin(timeline_addfeat/365*2*np.pi) # periodic & continuity\n",
    "timeline_addfeat = np.reshape(timeline_addfeat, (-1, 1))\n",
    "if (config[\"Flag_timeline_feature\"]):\n",
    "    feature_map = np.concatenate((time_structure, timeline_addfeat), axis = 1)\n",
    "else:\n",
    "    feature_map = time_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aadc46-e068-40aa-9498-17225f9be4c7",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b4c0fe-4d89-498e-9d2c-da01adde0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = feature_map[:train_size,:]; time_structure = time_structure[train_size:,:]\n",
    "valid_set = feature_map[:valid_size,:]; time_structure = time_structure[valid_size:,:]\n",
    "test_set  = feature_map[:test_size,:] ; time_structure = time_structure[test_size:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e84f027-a8f2-4203-a89e-cde624d2bdd9",
   "metadata": {},
   "source": [
    "### Rescale dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73a43875-8565-4b43-be73-4122c65ec27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_scaler.gz']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "valid_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "test_scaler  = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "train_set_scaled = train_scaler.fit_transform(train_set)\n",
    "valid_set_scaled = valid_scaler.fit_transform(valid_set)\n",
    "test_set_scaled  = test_scaler.fit_transform(test_set)\n",
    "\n",
    "os.chdir(config[\"Path_IVT_calculation\"])\n",
    "joblib.dump(train_scaler, 'train_scaler.gz')\n",
    "joblib.dump(valid_scaler, 'valid_scaler.gz')\n",
    "joblib.dump(test_scaler, 'test_scaler.gz')\n",
    "\n",
    "#my_scaler = joblib.load('scaler.gz')\n",
    "#scaler.inverse_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e15f4db-3dfd-4681-b15e-4a907c1f78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(config[\"Path_IVT_calculation\"])\n",
    "np.savez(config[\"ML_fname_dataset\"], \n",
    "         train = train_set_scaled, \n",
    "         valid = valid_set_scaled, \n",
    "         test  = test_set_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "889a3b03-b1f9-425d-b6d1-91c0a527e5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define \"available\" data index\\ndata_idx = np.arange(data_size)\\ntrain_data_idx = data_idx[:train_size]; data_idx = data_idx[train_size:]\\nvalid_data_idx = data_idx[:valid_size]; data_idx = data_idx[valid_size:]\\ntest_data_idx = data_idx[:test_size]; data_idx = data_idx[test_size:]\\n\\n\\n# Padding into sequence\\nX_train = np.zeros((train_size, config[\"ML_seq_len\"], feature_num))\\nY_train = np.zeros((train_size, 1, feature_num))\\nfor i, idx in enumerate(train_data_idx):\\n    X_train[i,:,:] = time_structure[idx:idx+config[\"ML_seq_len\"],:]\\n    Y_train[i,:,:] = time_structure[idx+config[\"ML_seq_len\"]+config[\"ML_forecast_step\"]-1,:].reshape(1, -1)\\n    \\nX_valid = np.zeros((valid_size, config[\"ML_seq_len\"], feature_num))\\nY_valid = np.zeros((valid_size, 1, feature_num))\\nfor i, idx in enumerate(valid_data_idx):\\n    X_valid[i,:,:] = time_structure[idx:idx+config[\"ML_seq_len\"],:]\\n    Y_valid[i,:,:] = time_structure[idx+config[\"ML_seq_len\"]+config[\"ML_forecast_step\"]-1,:].reshape(1, -1)\\n    \\n\\nX_test = np.zeros((test_size, config[\"ML_seq_len\"], feature_num))\\nY_test = np.zeros((test_size, 1, feature_num))\\nfor i, idx in enumerate(test_data_idx):\\n    X_test[i,:,:] = time_structure[idx:idx+config[\"ML_seq_len\"],:]\\n    Y_test[i,:,:] = time_structure[idx+config[\"ML_seq_len\"]+config[\"ML_forecast_step\"]-1,:].reshape(1, -1)\\n\\n# Data\\nX_train = torch.from_numpy(X_train).type(torch.FloatTensor)\\nY_train = torch.from_numpy(Y_train).type(torch.FloatTensor)\\nX_valid = torch.from_numpy(X_valid).type(torch.FloatTensor)\\nY_valid = torch.from_numpy(Y_valid).type(torch.FloatTensor)\\nX_test = torch.from_numpy(X_test).type(torch.FloatTensor)\\nY_test = torch.from_numpy(Y_test).type(torch.FloatTensor)\\ntrain_data_idx = torch.from_numpy(train_data_idx.reshape(-1, 1, 1)).type(torch.IntTensor)\\nvalid_data_idx = torch.from_numpy(valid_data_idx.reshape(-1, 1, 1)).type(torch.IntTensor)\\ntest_data_idx = torch.from_numpy(test_data_idx.reshape(-1, 1, 1)).type(torch.IntTensor)\\n# Encapsulate to TensorDataset\\ntrain_set = TensorDataset(X_train, Y_train, train_data_idx)\\nvalid_set = TensorDataset(X_valid, Y_valid, valid_data_idx)\\ntest_set = TensorDataset(X_test, Y_test, test_data_idx)\\n\\nprint(train_size)\\nprint(data_size)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Define \"available\" data index\n",
    "data_idx = np.arange(data_size)\n",
    "train_data_idx = data_idx[:train_size]; data_idx = data_idx[train_size:]\n",
    "valid_data_idx = data_idx[:valid_size]; data_idx = data_idx[valid_size:]\n",
    "test_data_idx = data_idx[:test_size]; data_idx = data_idx[test_size:]\n",
    "\n",
    "\n",
    "# Padding into sequence\n",
    "X_train = np.zeros((train_size, config[\"ML_seq_len\"], feature_num))\n",
    "Y_train = np.zeros((train_size, 1, feature_num))\n",
    "for i, idx in enumerate(train_data_idx):\n",
    "    X_train[i,:,:] = time_structure[idx:idx+config[\"ML_seq_len\"],:]\n",
    "    Y_train[i,:,:] = time_structure[idx+config[\"ML_seq_len\"]+config[\"ML_forecast_step\"]-1,:].reshape(1, -1)\n",
    "    \n",
    "X_valid = np.zeros((valid_size, config[\"ML_seq_len\"], feature_num))\n",
    "Y_valid = np.zeros((valid_size, 1, feature_num))\n",
    "for i, idx in enumerate(valid_data_idx):\n",
    "    X_valid[i,:,:] = time_structure[idx:idx+config[\"ML_seq_len\"],:]\n",
    "    Y_valid[i,:,:] = time_structure[idx+config[\"ML_seq_len\"]+config[\"ML_forecast_step\"]-1,:].reshape(1, -1)\n",
    "    \n",
    "\n",
    "X_test = np.zeros((test_size, config[\"ML_seq_len\"], feature_num))\n",
    "Y_test = np.zeros((test_size, 1, feature_num))\n",
    "for i, idx in enumerate(test_data_idx):\n",
    "    X_test[i,:,:] = time_structure[idx:idx+config[\"ML_seq_len\"],:]\n",
    "    Y_test[i,:,:] = time_structure[idx+config[\"ML_seq_len\"]+config[\"ML_forecast_step\"]-1,:].reshape(1, -1)\n",
    "\n",
    "# Data\n",
    "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "Y_train = torch.from_numpy(Y_train).type(torch.FloatTensor)\n",
    "X_valid = torch.from_numpy(X_valid).type(torch.FloatTensor)\n",
    "Y_valid = torch.from_numpy(Y_valid).type(torch.FloatTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "Y_test = torch.from_numpy(Y_test).type(torch.FloatTensor)\n",
    "train_data_idx = torch.from_numpy(train_data_idx.reshape(-1, 1, 1)).type(torch.IntTensor)\n",
    "valid_data_idx = torch.from_numpy(valid_data_idx.reshape(-1, 1, 1)).type(torch.IntTensor)\n",
    "test_data_idx = torch.from_numpy(test_data_idx.reshape(-1, 1, 1)).type(torch.IntTensor)\n",
    "# Encapsulate to TensorDataset\n",
    "train_set = TensorDataset(X_train, Y_train, train_data_idx)\n",
    "valid_set = TensorDataset(X_valid, Y_valid, valid_data_idx)\n",
    "test_set = TensorDataset(X_test, Y_test, test_data_idx)\n",
    "\n",
    "print(train_size)\n",
    "print(data_size)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pcore_env",
   "language": "python",
   "name": "pcore_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
