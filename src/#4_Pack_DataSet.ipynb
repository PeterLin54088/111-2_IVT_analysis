{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477435fb-45f6-4f80-bf8a-c6f7499a3681",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd011df-b141-4fa5-8380-dba0b177ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8959087-bf41-44f6-9f60-a13c23975cee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "834e4edf-f0c2-4e4a-b6f5-1d2cb81e9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config path\n",
    "root = '/Volumes/Expansion/User_Backup/b08209033/111-2_IVT_analysis/'\n",
    "file = 'config.json'\n",
    "config_path = os.path.join(root, file)\n",
    "\n",
    "# Import config\n",
    "with open(config_path) as infile:\n",
    "    config = json.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "# Update config\n",
    "config.update({\"Flag_timeline_feature\": False})\n",
    "config.update({\"ML_fname_dataset\": \"IVT_TS_dataset.npz\"})\n",
    "\n",
    "# Export config\n",
    "with open(config_path, 'w') as outfile:\n",
    "    json.dump(config, outfile, sort_keys=True)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f14cc-a6b3-4bee-8c63-98d9b0ba0fed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Read SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af15f15-4dcd-4851-92ce-73b10f85c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(config[\"Path_IVT_calculation\"])\n",
    "with np.load(config[\"Fname_IVT_svd\"]) as dataset:\n",
    "    time_structure = dataset['time']\n",
    "    feature_num = int(config[\"Var_Feature_num_SVD\"])\n",
    "# Choose k important structure\n",
    "time_structure = (time_structure[:feature_num]).T # (Timestep, feature) after transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5885d50-7b49-4900-ae88-7d7c2fb107f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab860e3-7d0c-4116-9265-3b1ebc95b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio\n",
    "    # Total: 16 partitions\n",
    "    # train: 12 partitions\n",
    "    # test :  3 partitions\n",
    "    # valid:  1 partitions\n",
    "data_ratio  = 16/16\n",
    "train_ratio = 12/16\n",
    "test_ratio  =  3/16\n",
    "valid_ratio =  1/16\n",
    "split_ratio = [train_ratio, valid_ratio, test_ratio]\n",
    "\n",
    "# Size\n",
    "data_size  = len(time_structure)\n",
    "train_size = int(data_size * train_ratio)\n",
    "test_size  = int(data_size * test_ratio)\n",
    "valid_size = data_size - (train_size + test_size) # remaining\n",
    "split_size = [train_size, valid_size, test_size]\n",
    "\n",
    "# Save data split info\n",
    "config.update({\"ML_split_ratio\": split_ratio})\n",
    "config.update({\"ML_split_size\":  split_size })\n",
    "with open(config_path, 'w') as outfile:\n",
    "    json.dump(config, outfile, sort_keys=True)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3add2c1-9c8c-4506-acee-a866a5cf3197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline feature (optional)\n",
    "    # This feature is designed to be \n",
    "    # a periodic sin wave, with period = 365 day\n",
    "timeline_addfeat = np.arange(data_size) # monotonic\n",
    "timeline_addfeat = np.abs(np.sin(timeline_addfeat/365*np.pi)) # periodic & continuity\n",
    "timeline_addfeat = np.reshape(timeline_addfeat, (-1, 1))\n",
    "\n",
    "if (config[\"Flag_timeline_feature\"]):\n",
    "    feature_map = np.concatenate((time_structure, timeline_addfeat), axis = 1)\n",
    "else:\n",
    "    feature_map = time_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54b4c0fe-4d89-498e-9d2c-da01adde0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_set = feature_map[:train_size,:]; time_structure = time_structure[train_size:,:]\n",
    "valid_set = feature_map[:valid_size,:]; time_structure = time_structure[valid_size:,:]\n",
    "test_set  = feature_map[:test_size,:] ; time_structure = time_structure[test_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a43875-8565-4b43-be73-4122c65ec27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "my_scaler = joblib.load('scaler.gz')\n",
    "scaler.inverse_transform(scaled_data)\n",
    "\"\"\"\n",
    "# Rescale dataset\n",
    "train_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "valid_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "test_scaler  = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "train_set_scaled = train_scaler.fit_transform(train_set)\n",
    "valid_set_scaled = valid_scaler.fit_transform(valid_set)\n",
    "test_set_scaled  = test_scaler.fit_transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e15f4db-3dfd-4681-b15e-4a907c1f78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler\n",
    "os.chdir(config[\"Path_IVT_calculation\"])\n",
    "joblib.dump(train_scaler, 'train_scaler.gz')\n",
    "joblib.dump(valid_scaler, 'valid_scaler.gz')\n",
    "joblib.dump(test_scaler, 'test_scaler.gz')\n",
    "# Save dataset\n",
    "os.chdir(config[\"Path_IVT_calculation\"])\n",
    "np.savez(config[\"ML_fname_dataset\"], \n",
    "         train = train_set_scaled, \n",
    "         valid = valid_set_scaled, \n",
    "         test  = test_set_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pcore_env",
   "language": "python",
   "name": "pcore_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
