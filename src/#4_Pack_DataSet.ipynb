{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477435fb-45f6-4f80-bf8a-c6f7499a3681",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd011df-b141-4fa5-8380-dba0b177ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "\n",
    "# Tensor\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8959087-bf41-44f6-9f60-a13c23975cee",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "834e4edf-f0c2-4e4a-b6f5-1d2cb81e9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config path\n",
    "root = '/Volumes/Expansion/User_Backup/b08209033/111-2_IVT_analysis/'\n",
    "folder = '2023_0330'\n",
    "file = 'src/config.json'\n",
    "config_path = os.path.join(os.path.join(root, folder), file)\n",
    "\n",
    "# Import config\n",
    "with open(config_path) as infile:\n",
    "    config = json.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "# Update config\n",
    "config.update({\"ML_seq_len\": 7})\n",
    "config.update({\"ML_forecast_step\": 1})\n",
    "config.update({\"ML_train_set_name\": 'TensorDataset_train.pt'})\n",
    "config.update({\"ML_valid_set_name\": 'TensorDataset_valid.pt'})\n",
    "config.update({\"ML_test_set_name\": 'TensorDataset_test.pt'})\n",
    "\n",
    "# Export config\n",
    "with open(config_path, 'w') as outfile:\n",
    "    json.dump(config, outfile, sort_keys=True)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f14cc-a6b3-4bee-8c63-98d9b0ba0fed",
   "metadata": {},
   "source": [
    "# Read time structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af15f15-4dcd-4851-92ce-73b10f85c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(config[\"IVTPath\"])\n",
    "with np.load(config[\"IVT_SVD_fname\"]) as dataset:\n",
    "    time_structure = dataset['time']\n",
    "    feature_num = int(dataset['feature_threshold'][0][0])\n",
    "\n",
    "# Choose k important structure\n",
    "time_structure = (time_structure[:feature_num]).T\n",
    "\n",
    "# Test\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "time_structure = scaler.fit_transform(time_structure)\n",
    "#scaler.inverse_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5885d50-7b49-4900-ae88-7d7c2fb107f0",
   "metadata": {},
   "source": [
    "# Pack into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889a3b03-b1f9-425d-b6d1-91c0a527e5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11766\n",
      "15688\n"
     ]
    }
   ],
   "source": [
    "# Defined train valid test split\n",
    "train_ratio = 12/16\n",
    "test_ratio = 3/16\n",
    "valid_ratio = 1 - train_ratio - test_ratio\n",
    "\n",
    "# Define \"available\" data length\n",
    "data_size = len(time_structure) - (config[\"ML_seq_len\"] + config[\"ML_forecast_step\"]) + 1\n",
    "train_size = int(data_size * train_ratio)\n",
    "test_size = int(data_size * test_ratio)\n",
    "valid_size = data_size - (train_size + test_size)\n",
    "\n",
    "# Define \"available\" data index\n",
    "data_idx = np.arange(data_size)\n",
    "train_data_idx = data_idx[:train_size]; data_idx = data_idx[train_size:]\n",
    "valid_data_idx = data_idx[:valid_size]; data_idx = data_idx[valid_size:]\n",
    "test_data_idx = data_idx[:test_size]; data_idx = data_idx[test_size:]\n",
    "\n",
    "\n",
    "# Padding into sequence\n",
    "X_train = np.zeros((train_size, config[\"ML_seq_len\"], feature_num))\n",
    "Y_train = np.zeros((train_size, 1, feature_num))\n",
    "for i, idx in enumerate(train_data_idx):\n",
    "    X_train[i,:,:] = time_structure[idx:idx+config[\"ML_seq_len\"],:]\n",
    "    Y_train[i,:,:] = time_structure[idx+config[\"ML_seq_len\"]+config[\"ML_forecast_step\"]-1,:].reshape(1, -1)\n",
    "    \n",
    "X_valid = np.zeros((valid_size, config[\"ML_seq_len\"], feature_num))\n",
    "Y_valid = np.zeros((valid_size, 1, feature_num))\n",
    "for i, idx in enumerate(valid_data_idx):\n",
    "    X_valid[i,:,:] = time_structure[idx:idx+config[\"ML_seq_len\"],:]\n",
    "    Y_valid[i,:,:] = time_structure[idx+config[\"ML_seq_len\"]+config[\"ML_forecast_step\"]-1,:].reshape(1, -1)\n",
    "    \n",
    "\n",
    "X_test = np.zeros((test_size, config[\"ML_seq_len\"], feature_num))\n",
    "Y_test = np.zeros((test_size, 1, feature_num))\n",
    "for i, idx in enumerate(test_data_idx):\n",
    "    X_test[i,:,:] = time_structure[idx:idx+config[\"ML_seq_len\"],:]\n",
    "    Y_test[i,:,:] = time_structure[idx+config[\"ML_seq_len\"]+config[\"ML_forecast_step\"]-1,:].reshape(1, -1)\n",
    "\n",
    "# Scaler, MinMax or Norm or else\n",
    "def Normalizer(tensor):\n",
    "    multiplier = 3\n",
    "    mean =  tensor.mean()\n",
    "    std = tensor.std()\n",
    "    return (tensor - mean)/(std*multiplier), np.array([mean, (std*multiplier)])\n",
    "def MinMaxer(tensor):\n",
    "    maximum = tensor.max()\n",
    "    minimum = tensor.min()\n",
    "    return (tensor - minimum)/(maximum - minimum), np.array([minimum, maximum])\n",
    "\"\"\"\n",
    "X_train, X_train_dist = Normalizer(X_train)\n",
    "Y_train, Y_train_dist = Normalizer(Y_train)\n",
    "X_valid, X_valid_dist = Normalizer(X_valid)\n",
    "Y_valid, Y_valid_dist = Normalizer(Y_valid)\n",
    "X_test, X_test_dist = Normalizer(X_test)\n",
    "Y_test, Y_test_dist = Normalizer(Y_test)\n",
    "print(X_test_dist)\n",
    "print(Y_test_dist)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "X_train, X_train_dist = MinMaxer(X_train)\n",
    "Y_train, Y_train_dist = MinMaxer(Y_train)\n",
    "X_valid, X_valid_dist = MinMaxer(X_valid)\n",
    "Y_valid, Y_valid_dist = MinMaxer(Y_valid)\n",
    "X_test, X_test_dist = MinMaxer(X_test)\n",
    "Y_test, Y_test_dist = MinMaxer(Y_test)\n",
    "\"\"\"\n",
    "# Data\n",
    "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "Y_train = torch.from_numpy(Y_train).type(torch.FloatTensor)\n",
    "X_valid = torch.from_numpy(X_valid).type(torch.FloatTensor)\n",
    "Y_valid = torch.from_numpy(Y_valid).type(torch.FloatTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "Y_test = torch.from_numpy(Y_test).type(torch.FloatTensor)\n",
    "train_data_idx = torch.from_numpy(train_data_idx.reshape(-1, 1, 1)).type(torch.IntTensor)\n",
    "valid_data_idx = torch.from_numpy(valid_data_idx.reshape(-1, 1, 1)).type(torch.IntTensor)\n",
    "test_data_idx = torch.from_numpy(test_data_idx.reshape(-1, 1, 1)).type(torch.IntTensor)\n",
    "# Encapsulate to TensorDataset\n",
    "train_set = TensorDataset(X_train, Y_train, train_data_idx)\n",
    "valid_set = TensorDataset(X_valid, Y_valid, valid_data_idx)\n",
    "test_set = TensorDataset(X_test, Y_test, test_data_idx)\n",
    "\n",
    "print(train_size)\n",
    "print(data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e15f4db-3dfd-4681-b15e-4a907c1f78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(config[\"IVTPath\"])\n",
    "torch.save(train_set, config[\"ML_train_set_name\"])\n",
    "torch.save(valid_set, config[\"ML_valid_set_name\"])\n",
    "torch.save(test_set, config[\"ML_test_set_name\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
