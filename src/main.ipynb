{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322072e3-6e26-410c-adfc-bca09189e6a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f678f266-2ddd-48a4-9a1a-4b044eba2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"6\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"6\" # export NUMEXPR_NUM_THREADS=6\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import math\n",
    "\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter\n",
    "from cartopy.mpl.ticker import LatitudeFormatter\n",
    "\n",
    "# ML\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7892ebe-ef6f-4cc0-b752-07c9d27e87f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd64861-660b-4bc8-8e60-9736409bcbb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config (environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f1128a-f358-4db5-be7d-590ff32a42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "# Fname \n",
    "config.update({\"FolderName\": \"2023_0310\"})\n",
    "\n",
    "# Fpath\n",
    "config.update({\"FolderPath\": \"/Volumes/Expansion/User_Backup/b08209033/111-2_IVT_analysis\"})\n",
    "config.update({\"SubFolderPath\": os.path.join(config[\"FolderPath\"], config[\"FolderName\"])})\n",
    "config.update({\"DataPath\": os.path.join(config[\"FolderPath\"], \"data\")})\n",
    "config.update({\"SrcPath\": os.path.join(config[\"SubFolderPath\"], \"src\")})\n",
    "config.update({\"ImgPath\": os.path.join(config[\"SubFolderPath\"], \"img\")})\n",
    "\n",
    "os.chdir(config[\"FolderPath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c343c60-f920-42c6-8f32-2ed87b82e577",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config (ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1521ed-e66a-4fe6-b959-6ccca5e06982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "configML = {\n",
    "    'seed': 9527,      # Your seed number, you can pick your lucky number. :)\n",
    "    'select_all': True,   # Whether to use all features.\n",
    "    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 100,     # Number of epochs.            \n",
    "    'batch_size': 1, \n",
    "    'learning_rate': 1e-5,\n",
    "    'weight_decay': 0,\n",
    "    'early_stop': 10,    # If model has not improved for this many consecutive epochs, stop training.     \n",
    "    'save_path': './models/model.ckpt'  # Your model will be saved here.\n",
    "}\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39884d86-9565-42b2-bef2-2c7f94206d97",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e1a8bc-b71c-48a6-b702-46e725c4880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFolder(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    #else:\n",
    "    #    print(\"Folder already existed.\")\n",
    "createFolder(config[\"FolderPath\"])\n",
    "createFolder(config[\"SrcPath\"])\n",
    "createFolder(config[\"ImgPath\"])\n",
    "createFolder(config[\"DataPath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a0e42d-7457-4844-bc88-5e951967682f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Self-defiend Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e929b4aa-1b1a-4c6e-b68c-2470fb73c856",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "284fb2c9-2973-4419-9133-c4d4236de8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input .nc object\n",
    "# Return IVT with dimensions (year, day, level, lat, lon)\n",
    "def getIVT(rootgrp, remove_trend=False):\n",
    "    \n",
    "    # Determine array shape\n",
    "        # Find region index\n",
    "    lon = rootgrp['lon'][:]\n",
    "    lat = rootgrp['lat'][:]\n",
    "        # Find level index\n",
    "    level = rootgrp['plev'][:]\n",
    "    \n",
    "    # Read variables in original file\n",
    "    u = rootgrp['u'][:,:,:,:].reshape(-1, 365, level.shape[0], lat.shape[0], lon.shape[0])\n",
    "    v = rootgrp['v'][:,:,:,:].reshape(-1, 365, level.shape[0], lat.shape[0], lon.shape[0])\n",
    "    q = rootgrp['q'][:,:,:,:].reshape(-1, 365, level.shape[0], lat.shape[0], lon.shape[0])\n",
    "    if (remove_trend != False):\n",
    "        # Read variables in long-time average trend file\n",
    "        u_trend = remove_trend['u'][:,:,:,:]\n",
    "        v_trend = remove_trend['v'][:,:,:,:]\n",
    "        q_trend = remove_trend['q'][:,:,:,:]\n",
    "    \n",
    "    # Calculate IVT\n",
    "        # Calculate interpolated value by averaging\n",
    "    interp_u = (u[:,:,1:,:,:] + u[:,:,:-1,:,:])/2\n",
    "    interp_v = (v[:,:,1:,:,:] + v[:,:,:-1,:,:])/2\n",
    "    interp_q = (q[:,:,1:,:,:] + q[:,:,:-1,:,:])/2\n",
    "        # hPa to Pa unit conversion\n",
    "    thickness = (level[:-1] - level[1:])\n",
    "        # Broadcast \n",
    "    thickness = thickness[np.newaxis, np.newaxis, :, np.newaxis, np.newaxis]\n",
    "        # Integrate over levels\n",
    "    IVT_u = np.sum((interp_u * interp_q * thickness)/9.81, axis = 2)\n",
    "    IVT_v = np.sum((interp_v * interp_q * thickness)/9.81, axis = 2)\n",
    "    \n",
    "    return lat, lon, np.array((IVT_u, IVT_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313492c8-9a21-4730-b5a9-7b1a4187a31e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53534d16-e58d-44fb-bbe6-1188f5fe37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IVTDataset(torch.utils.data.Dataset):\n",
    "    # x, y is defined over lead-lag-1\n",
    "    def __init__(self, x, y=None, seq_len=14):\n",
    "        if y is None:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = torch.FloatTensor(y)\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.seq_len = seq_len\n",
    "    \"\"\"\n",
    "    Modify here to implement seq_len\n",
    "    \"\"\"\n",
    "    # Using sequence to get window\n",
    "    # Padding if exceeded\n",
    "    def __getitem__(self, idx):\n",
    "        if idx + self.seq_len > self.__len__():\n",
    "            x_pad = torch.zeros(self.seq_len, self.x.size()[1])\n",
    "            y_pad = torch.zeros(self.seq_len, self.y.size()[1])\n",
    "            x_pad[:self.__len__()-idx,:] = self.x[idx:]\n",
    "            y_pad[:self.__len__()-idx,:] = self.y[idx:]\n",
    "            return x_pad, y_pad\n",
    "        else:\n",
    "            return self.x[idx:idx+self.seq_len], self.y[idx:idx+self.seq_len]\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c222925f-2833-4cc0-9050-04f595a539a1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e8e35e-f37a-4899-899a-51d23f19d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(My_Model, self).__init__()\n",
    "        self.GRU = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.input_length = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_size\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        output, hidden_state = self.GRU(x, hidden_state)\n",
    "        return output, hidden_state\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.num_layers, batch_size, self.hidden_dim).zero_()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51110786-0f14-480a-b0b9-010e8c364674",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7b74b0-cbe5-4182-8913-660dcf77f629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Takes 16.644 sec\n"
     ]
    }
   ],
   "source": [
    "# Defined dims\n",
    "TOTAL_DIM = 2\n",
    "TOTAL_YEAR = 43\n",
    "\n",
    "# Process\n",
    "count = time.time()\n",
    "os.chdir(config[\"DataPath\"])\n",
    "rootgrp = nc.Dataset(\"dataset.nc\")\n",
    "trend = nc.Dataset(\"seasonal_cycle.nc\")\n",
    "lat, lon, IVT = getIVT(rootgrp, trend)\n",
    "#lat, lon, IVT = getIVT(rootgrp)\n",
    "rootgrp.close()\n",
    "trend.close()\n",
    "print(f\"Takes {(time.time()-count):.3f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48131b-cd36-465a-a662-8681a3722d7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data processing (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd378cb6-9c5e-43dc-8883-ab6da1b6d6dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Takes 222.294 sec\n"
     ]
    }
   ],
   "source": [
    "count = time.time()\n",
    "# Time axis reshape\n",
    "data = IVT.reshape(TOTAL_DIM, -1, lat.shape[0], lon.shape[0])\n",
    "# Space axis reshape\n",
    "data = data.reshape(TOTAL_DIM, TOTAL_YEAR*365, lat.shape[0]*lon.shape[0])\n",
    "# Variable axis reshape\n",
    "data = np.concatenate((data[0,:,:],data[1,:,:]), axis = -1)\n",
    "# Transpose to (Space, Time) dimension\n",
    "data = data.transpose()\n",
    "# SVD\n",
    "u, s, vh = np.linalg.svd(data, full_matrices=False)\n",
    "print(f\"Takes {(time.time()-count):.3f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f77d63-1639-451e-9b7f-b82386ae00df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Variance (Explainability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f680020f-32de-49e7-8388-c97a8de2b1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 17 components explain 90.17% variance.\n"
     ]
    }
   ],
   "source": [
    "# Defined threshold\n",
    "threshold = 0.9\n",
    "\n",
    "# Calculate explainability\n",
    "variance = np.square(s)\n",
    "total_var = np.sum(variance)\n",
    "ith_var = 0\n",
    "feature_num = None\n",
    "for i, var_i in enumerate(variance):\n",
    "    ith_var += var_i/total_var\n",
    "    if (ith_var >= threshold):\n",
    "        print(f\"First {i+1} components explain {ith_var*100:.2f}% variance.\")\n",
    "        feature_num = i+1\n",
    "        break\n",
    "space_weights = np.copy((vh[:feature_num]))\n",
    "scalar_parameter_a = np.min(space_weights)\n",
    "scalar_parameter_b = np.max(space_weights) - np.min(space_weights)\n",
    "space_weights = (space_weights - scalar_parameter_a) / scalar_parameter_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79d625-d9f2-47d1-81ce-28f3e1987dec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Undetermined blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c44f379-9e5d-4994-a9e2-d3ea7a7fa256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined train valid test split\n",
    "train_ratio = 32\n",
    "valid_ratio = 3\n",
    "test_ratio = 8\n",
    "seq_len = 7\n",
    "# Split train_valid_test\n",
    "train_space_weights = (space_weights.T)[:365*train_ratio]\n",
    "valid_space_weights = (space_weights.T)[365*train_ratio:365*(train_ratio+valid_ratio)]\n",
    "test_space_weights = (space_weights.T)[365*(train_ratio+valid_ratio):]\n",
    "# Split x-y\n",
    "X_train, Y_train = train_space_weights[:-1], train_space_weights[1:]\n",
    "X_valid, Y_valid = valid_space_weights[:-1], valid_space_weights[1:]\n",
    "X_test, Y_test = test_space_weights[:-1], test_space_weights[1:]\n",
    "\n",
    "# Encapsulate to class\n",
    "train_dataset = IVTDataset(X_train, Y_train, seq_len = seq_len)\n",
    "valid_dataset = IVTDataset(X_valid, Y_valid, seq_len = seq_len)\n",
    "test_dataset = IVTDataset(X_test, Y_test, seq_len = seq_len)\n",
    "# Encapsulate to Dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size = configML['batch_size'], pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = configML['batch_size'], pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = configML['batch_size'], pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de5facef-56f3-40c2-b872-0c81bc617dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "    \n",
    "    # Pre train stage\n",
    "        # model_dict\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models') # Create directory of saving models.\n",
    "        # train parameter\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.L1Loss(reduction='mean').to(device)\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config['learning_rate'], weight_decay = config['weight_decay'])\n",
    "    # Thread numbers\n",
    "     \n",
    "    \n",
    "    # Training & Validating\n",
    "    mean_train_loss_record = np.array([])\n",
    "    mean_valid_loss_record = np.array([])\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training stage\n",
    "        # Init\n",
    "        model.train()\n",
    "        loss_record = []\n",
    "        hidden = model.init_hidden(config['batch_size'])\n",
    "        for x, y in train_loader:\n",
    "            # Reset gradient\n",
    "            optimizer.zero_grad()\n",
    "            # cuda if possible\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            # Modify batch size in hidden state\n",
    "            if(x.size()[0]!=hidden.size()[1]):\n",
    "                hidden = (hidden[:,hidden.size()[1]-x.size()[0]:,:])\n",
    "            # Forward propagation\n",
    "            pred, hidden = model(x, hidden)\n",
    "            # Calculate loss\n",
    "            loss = criterion(pred, y)\n",
    "            # Backward propagation\n",
    "            loss.backward()\n",
    "            # Update model parameter\n",
    "            optimizer.step()                    \n",
    "            step += 1\n",
    "            # Detach unused graph\n",
    "            hidden.detach_()\n",
    "            loss_record.append(loss.detach().item())\n",
    "        \n",
    "        # Train loss\n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "        \n",
    "        # Validating stage\n",
    "        # Init\n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        hidden = model.init_hidden(configML['batch_size'])\n",
    "        for x, y in valid_loader:\n",
    "            # cuda if possible\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            # Modify batch size in hidden state\n",
    "            if(x.size()[0]!=hidden.size()[1]):\n",
    "                hidden = (hidden[:,hidden.size()[1]-x.size()[0]:,:])\n",
    "            # Skip gradient update and backward propagation\n",
    "            with torch.no_grad():\n",
    "                # Forward propagation\n",
    "                pred, hidden = model(x, hidden)\n",
    "                # Calculate loss\n",
    "                loss = criterion(pred, y)\n",
    "            \n",
    "            # Detach loss\n",
    "            loss_record.append(loss.item())\n",
    "            \n",
    "        # Valid loss\n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)\n",
    "        \n",
    "        # Show progress\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.7f}, Valid loss: {mean_valid_loss:.7f}')\n",
    "        \n",
    "        # Save model parameter\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "            print('Saving model with loss {:.5f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "            \n",
    "        # Early stop\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2abbe4c1-9568-4308-9884-f3702d113a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]: Train loss: 0.1797925, Valid loss: 0.0606732\n",
      "Saving model with loss 0.06067...\n",
      "Epoch [2/100]: Train loss: 0.0586119, Valid loss: 0.0599759\n",
      "Saving model with loss 0.05998...\n",
      "Epoch [3/100]: Train loss: 0.0579648, Valid loss: 0.0593973\n",
      "Saving model with loss 0.05940...\n",
      "Epoch [4/100]: Train loss: 0.0573958, Valid loss: 0.0588710\n",
      "Saving model with loss 0.05887...\n",
      "Epoch [5/100]: Train loss: 0.0568656, Valid loss: 0.0583732\n",
      "Saving model with loss 0.05837...\n",
      "Epoch [6/100]: Train loss: 0.0563659, Valid loss: 0.0579118\n",
      "Saving model with loss 0.05791...\n",
      "Epoch [7/100]: Train loss: 0.0558904, Valid loss: 0.0574849\n",
      "Saving model with loss 0.05748...\n",
      "Epoch [8/100]: Train loss: 0.0554318, Valid loss: 0.0570782\n",
      "Saving model with loss 0.05708...\n",
      "Epoch [9/100]: Train loss: 0.0549827, Valid loss: 0.0566736\n",
      "Saving model with loss 0.05667...\n",
      "Epoch [10/100]: Train loss: 0.0545392, Valid loss: 0.0562749\n",
      "Saving model with loss 0.05627...\n",
      "Epoch [11/100]: Train loss: 0.0541016, Valid loss: 0.0558765\n",
      "Saving model with loss 0.05588...\n",
      "Epoch [12/100]: Train loss: 0.0536675, Valid loss: 0.0554781\n",
      "Saving model with loss 0.05548...\n",
      "Epoch [13/100]: Train loss: 0.0532368, Valid loss: 0.0550772\n",
      "Saving model with loss 0.05508...\n",
      "Epoch [14/100]: Train loss: 0.0528103, Valid loss: 0.0546752\n",
      "Saving model with loss 0.05468...\n",
      "Epoch [15/100]: Train loss: 0.0523871, Valid loss: 0.0542660\n",
      "Saving model with loss 0.05427...\n",
      "Epoch [16/100]: Train loss: 0.0519659, Valid loss: 0.0538623\n",
      "Saving model with loss 0.05386...\n",
      "Epoch [17/100]: Train loss: 0.0515475, Valid loss: 0.0534546\n",
      "Saving model with loss 0.05345...\n",
      "Epoch [18/100]: Train loss: 0.0511302, Valid loss: 0.0530413\n",
      "Saving model with loss 0.05304...\n",
      "Epoch [19/100]: Train loss: 0.0507129, Valid loss: 0.0526295\n",
      "Saving model with loss 0.05263...\n",
      "Epoch [20/100]: Train loss: 0.0502919, Valid loss: 0.0522111\n",
      "Saving model with loss 0.05221...\n",
      "Epoch [21/100]: Train loss: 0.0498636, Valid loss: 0.0517715\n",
      "Saving model with loss 0.05177...\n",
      "Epoch [22/100]: Train loss: 0.0494251, Valid loss: 0.0513203\n",
      "Saving model with loss 0.05132...\n",
      "Epoch [23/100]: Train loss: 0.0489811, Valid loss: 0.0508716\n",
      "Saving model with loss 0.05087...\n",
      "Epoch [24/100]: Train loss: 0.0485404, Valid loss: 0.0504260\n",
      "Saving model with loss 0.05043...\n",
      "Epoch [25/100]: Train loss: 0.0481026, Valid loss: 0.0499789\n",
      "Saving model with loss 0.04998...\n",
      "Epoch [26/100]: Train loss: 0.0476620, Valid loss: 0.0495324\n",
      "Saving model with loss 0.04953...\n",
      "Epoch [27/100]: Train loss: 0.0472144, Valid loss: 0.0490836\n",
      "Saving model with loss 0.04908...\n",
      "Epoch [28/100]: Train loss: 0.0467593, Valid loss: 0.0486372\n",
      "Saving model with loss 0.04864...\n",
      "Epoch [29/100]: Train loss: 0.0463036, Valid loss: 0.0481828\n",
      "Saving model with loss 0.04818...\n",
      "Epoch [30/100]: Train loss: 0.0458499, Valid loss: 0.0477422\n",
      "Saving model with loss 0.04774...\n",
      "Epoch [31/100]: Train loss: 0.0453990, Valid loss: 0.0472957\n",
      "Saving model with loss 0.04730...\n",
      "Epoch [32/100]: Train loss: 0.0449512, Valid loss: 0.0468482\n",
      "Saving model with loss 0.04685...\n",
      "Epoch [33/100]: Train loss: 0.0445067, Valid loss: 0.0464075\n",
      "Saving model with loss 0.04641...\n",
      "Epoch [34/100]: Train loss: 0.0440617, Valid loss: 0.0459640\n",
      "Saving model with loss 0.04596...\n",
      "Epoch [35/100]: Train loss: 0.0436137, Valid loss: 0.0455137\n",
      "Saving model with loss 0.04551...\n",
      "Epoch [36/100]: Train loss: 0.0431507, Valid loss: 0.0450492\n",
      "Saving model with loss 0.04505...\n",
      "Epoch [37/100]: Train loss: 0.0426739, Valid loss: 0.0445648\n",
      "Saving model with loss 0.04456...\n",
      "Epoch [38/100]: Train loss: 0.0421936, Valid loss: 0.0440728\n",
      "Saving model with loss 0.04407...\n",
      "Epoch [39/100]: Train loss: 0.0417227, Valid loss: 0.0436023\n",
      "Saving model with loss 0.04360...\n",
      "Epoch [40/100]: Train loss: 0.0412642, Valid loss: 0.0431473\n",
      "Saving model with loss 0.04315...\n",
      "Epoch [41/100]: Train loss: 0.0408195, Valid loss: 0.0427058\n",
      "Saving model with loss 0.04271...\n",
      "Epoch [42/100]: Train loss: 0.0403853, Valid loss: 0.0422912\n",
      "Saving model with loss 0.04229...\n",
      "Epoch [43/100]: Train loss: 0.0399608, Valid loss: 0.0418737\n",
      "Saving model with loss 0.04187...\n",
      "Epoch [44/100]: Train loss: 0.0395476, Valid loss: 0.0414576\n",
      "Saving model with loss 0.04146...\n",
      "Epoch [45/100]: Train loss: 0.0391422, Valid loss: 0.0410613\n",
      "Saving model with loss 0.04106...\n",
      "Epoch [46/100]: Train loss: 0.0387480, Valid loss: 0.0406722\n",
      "Saving model with loss 0.04067...\n",
      "Epoch [47/100]: Train loss: 0.0383653, Valid loss: 0.0402912\n",
      "Saving model with loss 0.04029...\n",
      "Epoch [48/100]: Train loss: 0.0379915, Valid loss: 0.0399257\n",
      "Saving model with loss 0.03993...\n",
      "Epoch [49/100]: Train loss: 0.0376273, Valid loss: 0.0395575\n",
      "Saving model with loss 0.03956...\n",
      "Epoch [50/100]: Train loss: 0.0372705, Valid loss: 0.0391927\n",
      "Saving model with loss 0.03919...\n",
      "Epoch [51/100]: Train loss: 0.0369155, Valid loss: 0.0388312\n",
      "Saving model with loss 0.03883...\n",
      "Epoch [52/100]: Train loss: 0.0365524, Valid loss: 0.0384466\n",
      "Saving model with loss 0.03845...\n",
      "Epoch [53/100]: Train loss: 0.0361656, Valid loss: 0.0380306\n",
      "Saving model with loss 0.03803...\n",
      "Epoch [54/100]: Train loss: 0.0357561, Valid loss: 0.0375972\n",
      "Saving model with loss 0.03760...\n",
      "Epoch [55/100]: Train loss: 0.0353625, Valid loss: 0.0372089\n",
      "Saving model with loss 0.03721...\n",
      "Epoch [56/100]: Train loss: 0.0350051, Valid loss: 0.0368536\n",
      "Saving model with loss 0.03685...\n",
      "Epoch [57/100]: Train loss: 0.0346752, Valid loss: 0.0365260\n",
      "Saving model with loss 0.03653...\n",
      "Epoch [58/100]: Train loss: 0.0343651, Valid loss: 0.0362133\n",
      "Saving model with loss 0.03621...\n",
      "Epoch [59/100]: Train loss: 0.0340700, Valid loss: 0.0359182\n",
      "Saving model with loss 0.03592...\n",
      "Epoch [60/100]: Train loss: 0.0337885, Valid loss: 0.0356398\n",
      "Saving model with loss 0.03564...\n",
      "Epoch [61/100]: Train loss: 0.0335189, Valid loss: 0.0353602\n",
      "Saving model with loss 0.03536...\n",
      "Epoch [62/100]: Train loss: 0.0332611, Valid loss: 0.0351013\n",
      "Saving model with loss 0.03510...\n",
      "Epoch [63/100]: Train loss: 0.0330141, Valid loss: 0.0348399\n",
      "Saving model with loss 0.03484...\n",
      "Epoch [64/100]: Train loss: 0.0327788, Valid loss: 0.0345927\n",
      "Saving model with loss 0.03459...\n",
      "Epoch [65/100]: Train loss: 0.0325522, Valid loss: 0.0343498\n",
      "Saving model with loss 0.03435...\n",
      "Epoch [66/100]: Train loss: 0.0323345, Valid loss: 0.0341138\n",
      "Saving model with loss 0.03411...\n",
      "Epoch [67/100]: Train loss: 0.0321271, Valid loss: 0.0338949\n",
      "Saving model with loss 0.03389...\n",
      "Epoch [68/100]: Train loss: 0.0319287, Valid loss: 0.0336859\n",
      "Saving model with loss 0.03369...\n",
      "Epoch [69/100]: Train loss: 0.0317401, Valid loss: 0.0334901\n",
      "Saving model with loss 0.03349...\n",
      "Epoch [70/100]: Train loss: 0.0315601, Valid loss: 0.0332917\n",
      "Saving model with loss 0.03329...\n",
      "Epoch [71/100]: Train loss: 0.0313870, Valid loss: 0.0331031\n",
      "Saving model with loss 0.03310...\n",
      "Epoch [72/100]: Train loss: 0.0312221, Valid loss: 0.0329177\n",
      "Saving model with loss 0.03292...\n",
      "Epoch [73/100]: Train loss: 0.0310653, Valid loss: 0.0327495\n",
      "Saving model with loss 0.03275...\n",
      "Epoch [74/100]: Train loss: 0.0309152, Valid loss: 0.0325927\n",
      "Saving model with loss 0.03259...\n",
      "Epoch [75/100]: Train loss: 0.0307727, Valid loss: 0.0324419\n",
      "Saving model with loss 0.03244...\n",
      "Epoch [76/100]: Train loss: 0.0306368, Valid loss: 0.0322988\n",
      "Saving model with loss 0.03230...\n",
      "Epoch [77/100]: Train loss: 0.0305075, Valid loss: 0.0321617\n",
      "Saving model with loss 0.03216...\n",
      "Epoch [78/100]: Train loss: 0.0303843, Valid loss: 0.0320251\n",
      "Saving model with loss 0.03203...\n",
      "Epoch [79/100]: Train loss: 0.0302674, Valid loss: 0.0318943\n",
      "Saving model with loss 0.03189...\n",
      "Epoch [80/100]: Train loss: 0.0301553, Valid loss: 0.0317699\n",
      "Saving model with loss 0.03177...\n",
      "Epoch [81/100]: Train loss: 0.0300491, Valid loss: 0.0316585\n",
      "Saving model with loss 0.03166...\n",
      "Epoch [82/100]: Train loss: 0.0299485, Valid loss: 0.0315490\n",
      "Saving model with loss 0.03155...\n",
      "Epoch [83/100]: Train loss: 0.0298520, Valid loss: 0.0314413\n",
      "Saving model with loss 0.03144...\n",
      "Epoch [84/100]: Train loss: 0.0297604, Valid loss: 0.0313415\n",
      "Saving model with loss 0.03134...\n",
      "Epoch [85/100]: Train loss: 0.0296728, Valid loss: 0.0312418\n",
      "Saving model with loss 0.03124...\n",
      "Epoch [86/100]: Train loss: 0.0295895, Valid loss: 0.0311488\n",
      "Saving model with loss 0.03115...\n",
      "Epoch [87/100]: Train loss: 0.0295100, Valid loss: 0.0310571\n",
      "Saving model with loss 0.03106...\n",
      "Epoch [88/100]: Train loss: 0.0294337, Valid loss: 0.0309698\n",
      "Saving model with loss 0.03097...\n",
      "Epoch [89/100]: Train loss: 0.0293611, Valid loss: 0.0308866\n",
      "Saving model with loss 0.03089...\n",
      "Epoch [90/100]: Train loss: 0.0292916, Valid loss: 0.0308061\n",
      "Saving model with loss 0.03081...\n",
      "Epoch [91/100]: Train loss: 0.0292250, Valid loss: 0.0307294\n",
      "Saving model with loss 0.03073...\n",
      "Epoch [92/100]: Train loss: 0.0291608, Valid loss: 0.0306573\n",
      "Saving model with loss 0.03066...\n",
      "Epoch [93/100]: Train loss: 0.0290997, Valid loss: 0.0305824\n",
      "Saving model with loss 0.03058...\n",
      "Epoch [94/100]: Train loss: 0.0290413, Valid loss: 0.0305126\n",
      "Saving model with loss 0.03051...\n",
      "Epoch [95/100]: Train loss: 0.0289847, Valid loss: 0.0304499\n",
      "Saving model with loss 0.03045...\n",
      "Epoch [96/100]: Train loss: 0.0289304, Valid loss: 0.0303872\n",
      "Saving model with loss 0.03039...\n",
      "Epoch [97/100]: Train loss: 0.0288779, Valid loss: 0.0303246\n",
      "Saving model with loss 0.03032...\n",
      "Epoch [98/100]: Train loss: 0.0288270, Valid loss: 0.0302656\n",
      "Saving model with loss 0.03027...\n",
      "Epoch [99/100]: Train loss: 0.0287779, Valid loss: 0.0302058\n",
      "Saving model with loss 0.03021...\n",
      "Epoch [100/100]: Train loss: 0.0287302, Valid loss: 0.0301501\n",
      "Saving model with loss 0.03015...\n"
     ]
    }
   ],
   "source": [
    "os.chdir(config[\"FolderPath\"])\n",
    "model = My_Model(input_size = feature_num,\n",
    "                 hidden_size = feature_num,\n",
    "                 num_layers = 1).to(device) # put your model and data on the same computation device.\n",
    "trainer(train_loader, valid_loader, model, configML, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de47d5b2-74a7-4ed0-81a5-197160974a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4421, 0.4003, 0.4769, 0.3974, 0.4366, 0.5368, 0.4437, 0.5206,\n",
      "          0.4685, 0.5146, 0.4882, 0.4545, 0.4595, 0.4734, 0.3668, 0.5476,\n",
      "          0.5435],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Set your model to evaluation mode.\n",
    "preds = []\n",
    "hidden = model.init_hidden(configML['batch_size'])\n",
    "for i, (x,y) in enumerate(test_loader):\n",
    "    x = x.to(device)                        \n",
    "    with torch.no_grad():\n",
    "        pred, hidden = model(x, hidden)\n",
    "        preds.append(pred.detach().cpu()[:,0,:])\n",
    "preds = torch.cat(preds, dim=0).numpy()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e600f6c-9ddf-48cf-aa3e-cd81d1707408",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(config[\"ImgPath\"])\n",
    "\n",
    "for idx in range(feature_num):\n",
    "    plt.figure(figsize=(12,8), dpi = 200)\n",
    "    plt.plot(preds[:,idx], label = \"predict\", zorder = 3)\n",
    "    plt.plot(Y_test[:,idx], label = \"target\", zorder = 2)\n",
    "    plt.xticks(np.arange(180,2920,365))\n",
    "    plt.legend(loc = 1, prop={'size': 20})\n",
    "    plt.title(f\"Time weight of filtered SVD Spatial mode:{idx}\")\n",
    "    plt.savefig(f\"Time series, mode:{idx}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541fdd01-3f0e-41d5-9d31-b71a3ed4f008",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Unused blocks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:default_env] *",
   "language": "python",
   "name": "conda-env-default_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
